{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfY0aTa9QsZy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_RtoOJgT1ga"
      },
      "outputs": [],
      "source": [
        "!pip install mne mne_connectivity torch-geometric -q\n",
        "\n",
        "#!pip install PyWavelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G8NRmyXKJYbm"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from mne.preprocessing import ICA\n",
        "from scipy.signal import hilbert\n",
        "\n",
        "class MemoryEfficientPLVProcessor:\n",
        "    \"\"\"\n",
        "    Memory-efficient processor that handles TMS events and resting state data\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, output_dir='plv_results', montage='standard_1020'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.montage = montage\n",
        "\n",
        "        # PLV computation parameters\n",
        "        self.freq_range = (0.1, 40)\n",
        "        self.epoch_length = 5.0   # Default window length (shorter for TMS)\n",
        "        self.overlap = 0\n",
        "\n",
        "    def scan_subjects(self):\n",
        "        \"\"\"Scan for subject folders\"\"\"\n",
        "        subjects = [d for d in self.root_dir.iterdir()\n",
        "                   if d.is_dir() and d.name.startswith('sub-')]\n",
        "        print(f\"Found {len(subjects)} subjects\")\n",
        "        return subjects\n",
        "\n",
        "    def _load_tms_events(self, file_path):\n",
        "        \"\"\"More robust TMS event loading\"\"\"\n",
        "        try:\n",
        "            events_path = file_path.parent / file_path.name.replace('_eeg.eeg', '_events.tsv')\n",
        "            events = pd.read_csv(events_path, sep='\\t')\n",
        "\n",
        "            # Handle multiple TMS event types\n",
        "            tms_events = events[events['trial_type'].str.contains('TMS|pulse|stim', case=False, na=False)]\n",
        "\n",
        "            if len(tms_events) == 0:\n",
        "                print(f\"No TMS events found in {events_path}\")\n",
        "                return None\n",
        "\n",
        "            # Convert to array with [onset, duration]\n",
        "            return tms_events[['onset', 'duration']].values.astype(float)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"TMS event loading failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_and_preprocess_eeg(self, file_path):\n",
        "        \"\"\"Memory-optimized EEG loading and preprocessing\"\"\"\n",
        "        try:\n",
        "            if file_path.suffix == '.eeg':\n",
        "                vhdr_path = file_path.with_suffix('.vhdr')\n",
        "                if not vhdr_path.exists():\n",
        "                    print(f\"Missing .vhdr file for {file_path.name}\")\n",
        "                    return None\n",
        "\n",
        "                # Load without preloading to save memory\n",
        "                print(f\"Loading {vhdr_path.name}...\")\n",
        "                raw = mne.io.read_raw_brainvision(vhdr_path, preload=False, verbose=False)\n",
        "\n",
        "                # Set montage\n",
        "                try:\n",
        "                    montage = mne.channels.make_standard_montage(self.montage)\n",
        "                    raw.set_montage(montage, match_case=False, verbose=False)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not set montage: {e}\")\n",
        "\n",
        "                # Load data and apply filters\n",
        "                print(\"Applying filters...\")\n",
        "                raw.load_data()\n",
        "                raw.filter(0.1, 40.0, n_jobs=1, verbose=False)\n",
        "                raw.notch_filter(50.0, n_jobs=1, verbose=False)\n",
        "\n",
        "                # Memory-efficient ICA\n",
        "                print(\"Running ICA...\")\n",
        "                n_components = min(15, len(raw.ch_names) - 2)  # Fewer components\n",
        "                ica = ICA(\n",
        "                    n_components=n_components,\n",
        "                    max_iter=300,  # Fewer iterations\n",
        "                    random_state=97,\n",
        "                    verbose=False\n",
        "                )\n",
        "\n",
        "                # Fit on decimated data\n",
        "                ica.fit(raw, reject_by_annotation=True, decim=3)\n",
        "\n",
        "                # Automatic artifact detection\n",
        "                if len(mne.pick_types(raw.info, eog=True)) > 0:\n",
        "                    eog_inds, eog_scores = ica.find_bads_eog(raw)\n",
        "                    ica.exclude = eog_inds\n",
        "\n",
        "                # Apply ICA\n",
        "                print(\"Applying ICA correction...\")\n",
        "                ica.apply(raw)\n",
        "\n",
        "                return raw\n",
        "\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {file_path}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "    '''\n",
        "    def compute_plv_for_window(self, data, sfreq):\n",
        "        \"\"\"Compute PLV matrix with robust error handling\"\"\"\n",
        "        try:\n",
        "            n_channels, n_samples = data.shape\n",
        "\n",
        "            # Create temporary raw object\n",
        "            info = mne.create_info(\n",
        "                ch_names=[f'CH{i}' for i in range(n_channels)],\n",
        "                sfreq=sfreq,\n",
        "                ch_types='eeg'\n",
        "            )\n",
        "            raw_temp = mne.io.RawArray(data, info, verbose=False)\n",
        "\n",
        "            # Create epochs\n",
        "            events = np.array([[0, 0, 1]])\n",
        "            epochs = mne.Epochs(\n",
        "                raw_temp,\n",
        "                events,\n",
        "                tmin=0,\n",
        "                tmax=(n_samples-1)/sfreq,\n",
        "                baseline=None,\n",
        "                preload=True,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Compute PLV\n",
        "            from mne_connectivity import spectral_connectivity_epochs\n",
        "            con = spectral_connectivity_epochs(\n",
        "                epochs,\n",
        "                method='plv',\n",
        "                fmin=self.freq_range[0],\n",
        "                fmax=self.freq_range[1],\n",
        "                sfreq=sfreq,\n",
        "                faverage=True,  # Average across frequencies\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Get dense symmetric matrix\n",
        "            plv_matrix = con.get_data(output='dense')[0]\n",
        "\n",
        "            # Ensure matrix is properly shaped\n",
        "            if plv_matrix.shape != (n_channels, n_channels):\n",
        "                print(f\"Warning: PLV shape {plv_matrix.shape}, expected {(n_channels, n_channels)}\")\n",
        "                plv_matrix = np.eye(n_channels)\n",
        "\n",
        "            return plv_matrix\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"PLV computation error: {e}\")\n",
        "            return np.eye(data.shape[0])  # Return identity matrix as fallback\n",
        "    '''\n",
        "\n",
        "    def compute_plv_for_window(self, data, sfreq):\n",
        "        analytic_signal = hilbert(data, axis=1)\n",
        "        phases = np.angle(analytic_signal)\n",
        "        phase_diff = phases[:, None, :] - phases[None, :, :]  # (n_chan, n_chan, n_samples)\n",
        "        plv_matrix = np.abs(np.mean(np.exp(1j * phase_diff), axis=-1))\n",
        "        return plv_matrix\n",
        "\n",
        "\n",
        "    def _process_sliding_windows(self, raw, subject_id, file_type):\n",
        "      \"\"\"Process with shape validation\"\"\"\n",
        "      data = raw.get_data()\n",
        "      sfreq = raw.info['sfreq']\n",
        "      window_samples = int(self.epoch_length * sfreq)\n",
        "\n",
        "      for i, start_idx in enumerate(range(0, data.shape[1] - window_samples, window_samples)):\n",
        "          window_data = data[:, start_idx:start_idx+window_samples]\n",
        "\n",
        "          # Skip empty/invalid windows\n",
        "          if window_data.size == 0:\n",
        "              print(f\"Skipping empty window {i}\")\n",
        "              continue\n",
        "\n",
        "          plv_matrix = self.compute_plv_for_window(window_data, sfreq)\n",
        "          self.save_plv_matrix(plv_matrix, raw.ch_names, subject_id, i, file_type)\n",
        "\n",
        "          if i % 5 == 0:\n",
        "              gc.collect()\n",
        "\n",
        "      return True\n",
        "\n",
        "    def _process_tms_windows(self, raw, subject_id, tms_events):\n",
        "        \"\"\"Process TMS events with artifact exclusion\"\"\"\n",
        "        sfreq = raw.info['sfreq']\n",
        "        success = False\n",
        "\n",
        "        for i, (onset, duration) in enumerate(tms_events):\n",
        "            try:\n",
        "                # Skip 100ms post-TMS to avoid artifact\n",
        "                start = onset + 0.1  # 100ms after TMS\n",
        "                end = start + 5.0    # 5-second window\n",
        "\n",
        "                # Get data (auto-rejects bad segments)\n",
        "                window = raw.copy().crop(start, end).get_data()\n",
        "\n",
        "                if window.size == 0:\n",
        "                    continue\n",
        "\n",
        "                plv = self.compute_plv_for_window(window, sfreq)\n",
        "                self.save_plv_matrix(plv, raw.ch_names, subject_id, i, 'tms')\n",
        "                success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed TMS window {i}: {e}\")\n",
        "\n",
        "        return success\n",
        "\n",
        "    def process_file_to_plv(self, file_path, subject_id, file_type='rest'):\n",
        "        \"\"\"Main processor with better TMS detection\"\"\"\n",
        "        print(f\"\\nProcessing {file_path.name} as {file_type}...\")\n",
        "\n",
        "        try:\n",
        "            raw = self.load_and_preprocess_eeg(file_path)\n",
        "            if raw is None:\n",
        "                return False\n",
        "\n",
        "            # Force TMS detection for files containing 'tms' in name\n",
        "            if 'tms' in file_path.name.lower():\n",
        "                file_type = 'tms'\n",
        "\n",
        "            if file_type == 'tms':\n",
        "                tms_events = self._load_tms_events(file_path)\n",
        "                if tms_events is not None:\n",
        "                    return self._process_tms_windows(raw, subject_id, tms_events)\n",
        "                else:\n",
        "                    print(\"No valid TMS events found, processing as continuous data\")\n",
        "\n",
        "            # Default sliding window processing\n",
        "            return self._process_sliding_windows(raw, subject_id, file_type)\n",
        "\n",
        "        finally:\n",
        "            if 'raw' in locals():\n",
        "                del raw\n",
        "            gc.collect()\n",
        "\n",
        "    def save_plv_matrix(self, plv_matrix, ch_names, subject_id, window_idx, file_type):\n",
        "        \"\"\"Save PLV matrix with proper shape handling\"\"\"\n",
        "        try:\n",
        "            # Ensure the matrix is square\n",
        "            if plv_matrix.shape[0] != plv_matrix.shape[1]:\n",
        "                print(f\"Warning: Non-square PLV matrix {plv_matrix.shape}, creating square matrix\")\n",
        "                n_channels = len(ch_names)\n",
        "                plv_matrix = np.eye(n_channels)  # Fallback to identity matrix\n",
        "\n",
        "            # Ensure matrix matches channel names length\n",
        "            if plv_matrix.shape[0] != len(ch_names):\n",
        "                print(f\"Shape mismatch: PLV {plv_matrix.shape} vs channels {len(ch_names)}. Adjusting...\")\n",
        "                min_dim = min(plv_matrix.shape[0], len(ch_names))\n",
        "                plv_matrix = plv_matrix[:min_dim, :min_dim]\n",
        "                ch_names = ch_names[:min_dim]\n",
        "\n",
        "            # Create and save DataFrame\n",
        "            filename = f\"sub-{subject_id}_type-{file_type}_window-{window_idx:04d}_plv.csv\"\n",
        "            filepath = self.output_dir / filename\n",
        "\n",
        "            pd.DataFrame(plv_matrix,\n",
        "                        columns=ch_names,\n",
        "                        index=ch_names).to_csv(filepath)\n",
        "\n",
        "            print(f\"Saved: {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving PLV matrix: {e}\")\n",
        "            # Save error log\n",
        "            with open(self.output_dir / 'error_log.txt', 'a') as f:\n",
        "                f.write(f\"Error saving {filename}: {str(e)}\\n\")\n",
        "\n",
        "    def process_all_subjects(self):\n",
        "        \"\"\"Process all subjects (unchanged)\"\"\"\n",
        "        subjects = self.scan_subjects()\n",
        "        processed_count = 0\n",
        "\n",
        "        for subject_path in subjects:\n",
        "            subject_id = subject_path.name.replace('sub-', '')\n",
        "            eeg_dir = subject_path / 'eeg'\n",
        "\n",
        "            if not eeg_dir.exists():\n",
        "                continue\n",
        "\n",
        "            # Process resting state\n",
        "            for rest_file in eeg_dir.glob('*rest_eeg*.eeg'):\n",
        "                if self.process_file_to_plv(rest_file, subject_id, 'rest'):\n",
        "                    processed_count += 1\n",
        "\n",
        "            # Process TMS files\n",
        "            for tms_file in eeg_dir.glob('*tmseeg1_eeg.eeg'):\n",
        "                if self.process_file_to_plv(tms_file, subject_id, 'tms'):\n",
        "                    processed_count += 1\n",
        "\n",
        "            gc.collect()\n",
        "            print(f\"Completed subject {subject_id}\")\n",
        "\n",
        "        print(f\"Processed {processed_count} files\")\n",
        "\n",
        "# Rest of your code remains unchanged (PLVDatasetFromDisk and MemoryEfficientTMSClassifier classes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    processor = MemoryEfficientPLVProcessor(\n",
        "        root_dir=\"EEGTMS\",\n",
        "        output_dir=\"plv_matrices_single\"\n",
        "    )\n",
        "    processor.process_all_subjects()\n",
        "\n",
        "    classifier = MemoryEfficientTMSClassifier(\"plv_matrices_single\")\n",
        "    classifier.load_dataset()\n",
        "    classifier.train(epochs=50, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeDI-tbYXVtL"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import mne\n",
        "from mne_connectivity import spectral_connectivity_epochs\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mne.preprocessing import ICA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhsZs8sZXG3q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class EEGDataLoader:\n",
        "    \"\"\"\n",
        "    Loads EEG time series data organized in subject folders\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, montage='standard_1020'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.montage = montage\n",
        "        self.subjects = []\n",
        "        self.resting_data = {}\n",
        "        self.tms_data = {}\n",
        "        self.events_data = {}\n",
        "\n",
        "    def scan_subjects(self):\n",
        "        \"\"\"Scan for subject folders following sub-xxx pattern\"\"\"\n",
        "        self.subjects = [d for d in self.root_dir.iterdir()\n",
        "                        if d.is_dir() and d.name.startswith('sub-')]\n",
        "        print(f\"Found {len(self.subjects)} subjects\")\n",
        "        return self.subjects\n",
        "\n",
        "    def load_eeg_file(self, file_path):\n",
        "        \"\"\"Load .eeg EEG file using MNE and preprocess\"\"\"\n",
        "        try:\n",
        "            if file_path.suffix == '.eeg':\n",
        "                # BrainVision requires .vhdr file; .eeg and .vmrk must be present in the same folder\n",
        "                vhdr_path = file_path.with_suffix('.vhdr')\n",
        "                if not vhdr_path.exists():\n",
        "                    print(f\"Missing corresponding .vhdr file for {file_path.name}\")\n",
        "                    return None\n",
        "\n",
        "                # Load the raw data using the .vhdr file\n",
        "                raw = mne.io.read_raw_brainvision(vhdr_path, preload=True, verbose=False)\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {file_path}\")\n",
        "                return None\n",
        "\n",
        "            # Attempt to set standard montage\n",
        "            try:\n",
        "                montage = mne.channels.make_standard_montage(self.montage)\n",
        "                raw.set_montage(montage, match_case=False, verbose=False)\n",
        "            except Exception as montage_error:\n",
        "                print(f\"Warning: Could not set montage for {file_path}: {montage_error}\")\n",
        "\n",
        "            # Apply preprocessing\n",
        "            raw = self.preprocess_eeg(raw)\n",
        "            return raw\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_eeg(self, raw, l_freq=0.1, h_freq=40.0, notch_freq=50.0,\n",
        "                      apply_ica=True, n_components=0.95):\n",
        "        \"\"\"Bandpass filter and ICA motion correction for EEG\"\"\"\n",
        "        # Bandpass filtering\n",
        "        raw.filter(l_freq=l_freq, h_freq=h_freq, fir_design='firwin', verbose=False)\n",
        "\n",
        "        # Notch filter (optional)\n",
        "        if notch_freq:\n",
        "            raw.notch_filter(freqs=notch_freq, fir_design='firwin', verbose=False)\n",
        "\n",
        "        # Run ICA if requested\n",
        "        if apply_ica:\n",
        "            n_components = min(20, len(raw.ch_names) - 1)  # Use 20 components or channels-1 instead of 0.95\n",
        "            ica = mne.preprocessing.ICA(\n",
        "              n_components=n_components,\n",
        "              #method='picard',  # More stable than default\n",
        "              max_iter=500,\n",
        "              random_state=97,\n",
        "              verbose=False\n",
        "            )\n",
        "\n",
        "            ica.fit(raw, reject_by_annotation=True)\n",
        "\n",
        "            # Automatic artifact detection\n",
        "            if len(mne.pick_types(raw.info, eog=True)) > 0:\n",
        "                  eog_inds, eog_scores = ica.find_bads_eog(raw)\n",
        "                  ica.exclude = eog_inds\n",
        "\n",
        "            # Apply ICA\n",
        "            raw = ica.apply(raw)\n",
        "\n",
        "        return raw\n",
        "\n",
        "    def process_large_eeg(self, file_path, chunk_size=300):\n",
        "        \"\"\"Process EEG in chunks with robust time handling and memory management\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to BrainVision EEG file (.vhdr)\n",
        "            chunk_size: Duration of chunks in seconds\n",
        "\n",
        "        Yields:\n",
        "            Preprocessed Raw objects for each chunk\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If file cannot be read or chunk_size is invalid\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1. Load file without preloading\n",
        "            raw = mne.io.read_raw_brainvision(file_path, preload=False, verbose=False)\n",
        "\n",
        "            # 2. Calculate duration with floating-point precision\n",
        "            total_samples = raw.n_times\n",
        "            sfreq = raw.info['sfreq']\n",
        "            total_duration = total_samples / sfreq\n",
        "\n",
        "            # 3. Validate chunk size\n",
        "            if chunk_size <= 0:\n",
        "                raise ValueError(f\"chunk_size must be >0 (got {chunk_size})\")\n",
        "            if chunk_size > total_duration:\n",
        "                print(f\"Warning: chunk_size {chunk_size}s > file duration {total_duration:.2f}s. Using full duration.\")\n",
        "                chunk_size = total_duration\n",
        "\n",
        "            # 4. Process in chunks with safe cropping\n",
        "            for start in np.arange(0, total_duration, chunk_size):\n",
        "                try:\n",
        "                    # Calculate end time with 1 sample buffer\n",
        "                    end = min(start + chunk_size, total_duration - 1/sfreq)\n",
        "\n",
        "                    # Handle floating-point precision issues\n",
        "                    end = np.round(end, decimals=5)\n",
        "\n",
        "                    # Crop and load chunk\n",
        "                    with mne.utils.use_log_level('ERROR'):  # Suppress crop warnings\n",
        "                        raw_chunk = raw.copy().crop(start, end, include_tmax=False).load_data()\n",
        "\n",
        "                    # Process chunk\n",
        "                    processed_chunk = self.preprocess_eeg(raw_chunk)\n",
        "\n",
        "                    # Clear intermediate objects\n",
        "                    del raw_chunk\n",
        "\n",
        "                    yield processed_chunk\n",
        "\n",
        "                except Exception as chunk_error:\n",
        "                    print(f\"Error processing chunk {start:.1f}-{end:.1f}s: {str(chunk_error)}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as file_error:\n",
        "            raise ValueError(f\"Error processing {file_path}: {str(file_error)}\")\n",
        "\n",
        "        finally:\n",
        "            # Ensure raw file is closed\n",
        "            if 'raw' in locals():\n",
        "                raw.close()\n",
        "            gc.collect()\n",
        "\n",
        "    def load_events_file(self, file_path):\n",
        "        \"\"\"Load events from TSV file\"\"\"\n",
        "        try:\n",
        "            events_df = pd.read_csv(file_path, sep='\\t')\n",
        "            return events_df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading events {file_path}: {e}\")\n",
        "            return None\n",
        "    '''\n",
        "    def load_subject_data(self, subject_path):\n",
        "        \"\"\"Load all data for a single subject\"\"\"\n",
        "        eeg_dir = subject_path / 'eeg'\n",
        "        if not eeg_dir.exists():\n",
        "            print(f\"No EEG directory found for {subject_path.name}\")\n",
        "            return None, None, None\n",
        "\n",
        "        resting_files = list(eeg_dir.glob('*rest_eeg*.eeg'))\n",
        "        tms_files = list(eeg_dir.glob('*tmseeg1_eeg.eeg'))\n",
        "        event_files = list(eeg_dir.glob('*.tsv'))\n",
        "\n",
        "        resting_data = None\n",
        "        tms_data = None\n",
        "        events_data = None\n",
        "\n",
        "        # Load resting state data\n",
        "        if resting_files:\n",
        "            print(resting_files[0])\n",
        "            resting_data = self.load_eeg_file(resting_files[0])\n",
        "\n",
        "        # Load TMS data\n",
        "        if tms_files:\n",
        "            print(tms_files[0])\n",
        "            tms_data = self.load_eeg_file(tms_files[0])\n",
        "\n",
        "        # Load events\n",
        "        if event_files:\n",
        "            events_data = self.load_events_file(event_files[0])\n",
        "\n",
        "        return resting_data, tms_data, events_data\n",
        "    '''\n",
        "    def load_subject_data(self, subject_path):\n",
        "        \"\"\"Load all data for a single subject using chunked processing\"\"\"\n",
        "        eeg_dir = subject_path / 'eeg'\n",
        "        if not eeg_dir.exists():\n",
        "            print(f\"No EEG directory found for {subject_path.name}\")\n",
        "            return None, None, None\n",
        "\n",
        "        resting_files = list(eeg_dir.glob('*rest_eeg*.eeg'))\n",
        "        tms_files = list(eeg_dir.glob('*tmseeg1_eeg.eeg'))\n",
        "        event_files = list(eeg_dir.glob('*.tsv'))\n",
        "\n",
        "        # Process resting data in chunks\n",
        "        resting_chunks = []\n",
        "        if resting_files:\n",
        "            print(resting_files)\n",
        "            vhdr_path = resting_files[0].with_suffix('.vhdr')\n",
        "            for chunk in self.process_large_eeg(vhdr_path):\n",
        "                resting_chunks.append(chunk)\n",
        "\n",
        "        # Process TMS data in chunks\n",
        "        tms_chunks = []\n",
        "        if tms_files:\n",
        "            print(tms_files)\n",
        "            vhdr_path = tms_files[0].with_suffix('.vhdr')\n",
        "            for chunk in self.process_large_eeg(vhdr_path):\n",
        "                tms_chunks.append(chunk)\n",
        "\n",
        "        # Load events\n",
        "        events_data = None\n",
        "        if event_files:\n",
        "            events_data = self.load_events_file(event_files[0])\n",
        "\n",
        "        return resting_chunks, tms_chunks, events_data\n",
        "\n",
        "    def load_all_subjects(self):\n",
        "        \"\"\"Load data for all subjects\"\"\"\n",
        "        self.scan_subjects()\n",
        "\n",
        "        for subject_path in self.subjects:\n",
        "            subject_id = subject_path.name\n",
        "            print(f\"Loading {subject_id}...\")\n",
        "\n",
        "            resting, tms, events = self.load_subject_data(subject_path)\n",
        "\n",
        "            if resting is not None:\n",
        "                self.resting_data[subject_id] = resting\n",
        "            if tms is not None:\n",
        "                self.tms_data[subject_id] = tms\n",
        "            if events is not None:\n",
        "                self.events_data[subject_id] = events\n",
        "\n",
        "        print(f\"Loaded resting data for {len(self.resting_data)} subjects\")\n",
        "        print(f\"Loaded TMS data for {len(self.tms_data)} subjects\")\n",
        "        print(f\"Loaded events data for {len(self.events_data)} subjects\")\n",
        "\n",
        "class PLVGraphConstructor:\n",
        "    \"\"\"\n",
        "    Constructs PLV-based connectivity graphs from EEG data using MNE-Connectivity\n",
        "    \"\"\"\n",
        "    def __init__(self, freq_bands=None, threshold_percentile=90, epoch_length=1.0, output_dir='/content/drive/MyDrive/plv_results'):\n",
        "        if freq_bands is None:\n",
        "            self.freq_bands = {\n",
        "                'theta': (4, 8),\n",
        "                'alpha': (8, 13),\n",
        "                'beta': (13, 30),\n",
        "                'gamma': (30, 40)\n",
        "            }\n",
        "        else:\n",
        "            self.freq_bands = freq_bands\n",
        "        self.threshold_percentile = threshold_percentile\n",
        "        self.epoch_length = epoch_length\n",
        "\n",
        "        # Create output directory\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def save_plv_matrix(self, plv_matrix, subject_id, window_idx, band_name):\n",
        "        \"\"\"Save PLV matrix to CSV\"\"\"\n",
        "        # Create filename\n",
        "        filename = f\"sub-{subject_id}_window-{window_idx}_{band_name}_plv.csv\"\n",
        "        filepath = self.output_dir / filename\n",
        "        print(\"saving \"+ filepath)\n",
        "        # Convert to DataFrame and save\n",
        "        df = pd.DataFrame(plv_matrix,\n",
        "                         columns=self.ch_names,\n",
        "                         index=self.ch_names)\n",
        "        df.to_csv(filepath)\n",
        "        return filepath\n",
        "\n",
        "    def create_multi_band_edge_index(self, raw_data, subject_id=None, window_idx=None):\n",
        "        \"\"\"\n",
        "        Create edge indices for multiple frequency bands and save PLV matrices\n",
        "        \"\"\"\n",
        "        edge_indices = {}\n",
        "        plv_matrices = {}\n",
        "\n",
        "        # Store channel names for CSV headers\n",
        "        self.ch_names = raw_data.ch_names\n",
        "\n",
        "        for band_name, freq_band in self.freq_bands.items():\n",
        "            try:\n",
        "                adj_matrix, plv_matrix = self.create_plv_adjacency_matrix(raw_data, freq_band)\n",
        "\n",
        "                # Save PLV matrix to CSV if subject/window info provided\n",
        "                print(\"presaving\")\n",
        "                if subject_id is not None and window_idx is not None:\n",
        "                    self.save_plv_matrix(plv_matrix, subject_id, window_idx, band_name)\n",
        "\n",
        "                # Convert to edge index format\n",
        "                edges = np.where(adj_matrix > 0)\n",
        "                if len(edges[0]) > 0:\n",
        "                    edge_index = np.stack([edges[0], edges[1]], axis=0)\n",
        "                    edge_indices[band_name] = torch.tensor(edge_index, dtype=torch.long)\n",
        "                else:\n",
        "                    n_channels = adj_matrix.shape[0]\n",
        "                    edge_index = np.array([[i, i+1] for i in range(n_channels-1)]).T\n",
        "                    edge_indices[band_name] = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "                plv_matrices[band_name] = plv_matrix\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing band {band_name}: {e}\")\n",
        "                n_channels = len(raw_data.ch_names)\n",
        "                edge_index = np.array([[i, i+1] for i in range(n_channels-1)]).T\n",
        "                edge_indices[band_name] = torch.tensor(edge_index, dtype=torch.long)\n",
        "                plv_matrices[band_name] = np.eye(n_channels)\n",
        "\n",
        "        return edge_indices, plv_matrices\n",
        "\n",
        "class TMSEEGDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for TMS-EEG classification with PLV-based connectivity\n",
        "    \"\"\"\n",
        "    def __init__(self, eeg_data, events_data, window_size=1000, overlap=0.5, freq_bands=None):\n",
        "        self.eeg_data = eeg_data\n",
        "        self.events_data = events_data\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.edge_indices = []\n",
        "        self.plv_features = []\n",
        "\n",
        "        if freq_bands is None:\n",
        "            self.freq_bands = {\n",
        "                'theta': (4, 8),\n",
        "                'alpha': (8, 13),\n",
        "                'beta': (13, 30),\n",
        "                'gamma': (30, 40)\n",
        "            }\n",
        "        else:\n",
        "            self.freq_bands = freq_bands\n",
        "\n",
        "        self._prepare_samples()\n",
        "    '''\n",
        "    def _prepare_samples(self):\n",
        "        \"\"\"Prepare windowed samples with PLV-based connectivity using MNE-Connectivity\"\"\"\n",
        "        graph_constructor = PLVGraphConstructor(freq_bands=self.freq_bands)\n",
        "\n",
        "        for subject_id in self.eeg_data.keys():\n",
        "            raw = self.eeg_data[subject_id]\n",
        "            events = self.events_data.get(subject_id, None)\n",
        "\n",
        "            # Get EEG data and sampling frequency\n",
        "            data = raw.get_data()  # Shape: (n_channels, n_timepoints)\n",
        "            sfreq = raw.info['sfreq']\n",
        "            n_channels, n_timepoints = data.shape\n",
        "\n",
        "            # Create sliding windows\n",
        "            step_size = int(self.window_size * (1 - self.overlap))\n",
        "\n",
        "            for start_idx in range(0, n_timepoints - self.window_size, step_size):\n",
        "                end_idx = start_idx + self.window_size\n",
        "                window_data = data[:, start_idx:end_idx]\n",
        "\n",
        "                # Create a temporary Raw object for this window\n",
        "                info = raw.info.copy()\n",
        "                raw_window = mne.io.RawArray(window_data, info, verbose=False)\n",
        "\n",
        "                # Compute PLV-based connectivity for this window using MNE-Connectivity\n",
        "                edge_indices, plv_matrices = graph_constructor.create_multi_band_edge_index(raw_window)\n",
        "\n",
        "                # Determine label based on events\n",
        "                label = self._get_window_label(start_idx, end_idx, events, sfreq)\n",
        "\n",
        "                self.samples.append(torch.tensor(window_data, dtype=torch.float32))\n",
        "                self.labels.append(label)\n",
        "                self.edge_indices.append(edge_indices)\n",
        "                self.plv_features.append(plv_matrices)\n",
        "    '''\n",
        "\n",
        "    def _prepare_samples(self):\n",
        "      \"\"\"Prepare windowed samples from chunked data\"\"\"\n",
        "      graph_constructor = PLVGraphConstructor(freq_bands=self.freq_bands)\n",
        "\n",
        "      for subject_id in self.eeg_data.keys():\n",
        "        # Extract subject number if using BIDS format (sub-XXX)\n",
        "        sub_num = subject_id.split('-')[-1] if 'sub-' in subject_id else subject_id\n",
        "\n",
        "        for chunk_idx, raw_chunk in enumerate(self.eeg_data[subject_id]):\n",
        "            data = raw_chunk.get_data()\n",
        "            sfreq = raw_chunk.info['sfreq']\n",
        "            n_channels, n_timepoints = data.shape\n",
        "\n",
        "            # Create sliding windows\n",
        "            step_size = int(self.window_size * (1 - self.overlap))\n",
        "\n",
        "            for window_idx, start_idx in enumerate(range(0, n_timepoints - self.window_size, step_size)):\n",
        "                end_idx = start_idx + self.window_size\n",
        "                window_data = data[:, start_idx:end_idx]\n",
        "\n",
        "                # Create temporary Raw object\n",
        "                info = raw_chunk.info.copy()\n",
        "                raw_window = mne.io.RawArray(window_data, info, verbose=False)\n",
        "\n",
        "                # Compute connectivity and save PLV matrices\n",
        "                edge_indices, plv_matrices = graph_constructor.create_multi_band_edge_index(\n",
        "                    raw_window,\n",
        "                    subject_id=sub_num,\n",
        "                    window_idx=window_idx\n",
        "                )\n",
        "\n",
        "                # Determine label\n",
        "                label = self._get_window_label(start_idx, end_idx,\n",
        "                                              self.events_data.get(subject_id, None),\n",
        "                                              sfreq)\n",
        "\n",
        "                self.samples.append(torch.tensor(window_data, dtype=torch.float32))\n",
        "                self.labels.append(label)\n",
        "                self.edge_indices.append(edge_indices)\n",
        "                self.plv_features.append(plv_matrices)\n",
        "\n",
        "    def _get_window_label(self, start_idx, end_idx, events, sfreq):\n",
        "        \"\"\"Determine label for a time window based on events\"\"\"\n",
        "        if events is None:\n",
        "            return 0  # Default to resting state\n",
        "\n",
        "        start_time = start_idx / sfreq\n",
        "        end_time = end_idx / sfreq\n",
        "\n",
        "        # Check if any TMS event falls within this window\n",
        "        for _, event in events.iterrows():\n",
        "            event_time = event.get('onset', 0)\n",
        "            if start_time <= event_time <= end_time:\n",
        "                return 1  # TMS event present\n",
        "\n",
        "        return 0  # No TMS event\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'x': self.samples[idx],\n",
        "            'edge_indices': self.edge_indices[idx],\n",
        "            'plv_features': self.plv_features[idx],\n",
        "            'y': self.labels[idx]\n",
        "        }\n",
        "\n",
        "class MultiBandTemporalAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-band temporal attention mechanism incorporating PLV features\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, num_heads=8, num_bands=4):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_bands = num_bands\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        # Separate attention for each frequency band\n",
        "        self.band_attentions = nn.ModuleList([\n",
        "            nn.MultiheadAttention(hidden_dim, num_heads, dropout=0.1, batch_first=True)\n",
        "            for _ in range(num_bands)\n",
        "        ])\n",
        "\n",
        "        # PLV feature integration\n",
        "        self.plv_projection = nn.Linear(1, hidden_dim // 4)\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Output fusion\n",
        "        self.fusion = nn.Linear(hidden_dim * num_bands + hidden_dim // 4, hidden_dim)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, plv_features):\n",
        "        # x shape: (batch_size, n_channels, seq_len)\n",
        "        batch_size, n_channels, seq_len = x.shape\n",
        "\n",
        "        # Project input to hidden dimension\n",
        "        x_proj = self.input_projection(x.transpose(1, 2))  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # Process each frequency band\n",
        "        band_outputs = []\n",
        "        for i, attention in enumerate(self.band_attentions):\n",
        "            # Apply attention for each band\n",
        "            attended, _ = attention(x_proj, x_proj, x_proj)\n",
        "            band_outputs.append(attended)\n",
        "\n",
        "        # Concatenate band outputs\n",
        "        multi_band_output = torch.cat(band_outputs, dim=-1)  # (batch_size, seq_len, hidden_dim * num_bands)\n",
        "\n",
        "        # Process PLV features\n",
        "        if plv_features is not None:\n",
        "            # Aggregate PLV features across bands\n",
        "            plv_vals = []\n",
        "            for band_name in ['theta', 'alpha', 'beta', 'gamma']:\n",
        "                if band_name in plv_features:\n",
        "                    plv_matrix = plv_features[band_name]\n",
        "                    # Take mean PLV as global connectivity measure\n",
        "                    mean_plv = torch.tensor(np.mean(plv_matrix), dtype=torch.float32).view(1, 1)\n",
        "                    plv_vals.append(mean_plv)\n",
        "\n",
        "            if plv_vals:\n",
        "                plv_tensor = torch.cat(plv_vals, dim=0).mean().view(1, 1, 1)  # (1, 1, 1)\n",
        "                plv_tensor = plv_tensor.expand(batch_size, seq_len, 1)  # (batch_size, seq_len, 1)\n",
        "                plv_features_proj = self.plv_projection(plv_tensor)  # (batch_size, seq_len, hidden_dim//4)\n",
        "\n",
        "                # Combine multi-band output with PLV features\n",
        "                combined = torch.cat([multi_band_output, plv_features_proj], dim=-1)\n",
        "            else:\n",
        "                combined = multi_band_output\n",
        "        else:\n",
        "            combined = multi_band_output\n",
        "\n",
        "        # Fusion and normalization\n",
        "        output = self.fusion(combined)\n",
        "        output = self.layer_norm(output)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output.transpose(1, 2)  # Return to (batch_size, n_channels, hidden_dim)\n",
        "\n",
        "class PLVSpatioTemporalGAT(nn.Module):\n",
        "    \"\"\"\n",
        "    PLV-based Spatio-Temporal Graph Attention Network for TMS-EEG classification\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels, seq_len, hidden_dim=64, num_heads=8, num_gat_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.seq_len = seq_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.freq_bands = ['theta', 'alpha', 'beta', 'gamma']\n",
        "\n",
        "        # Multi-band temporal attention with PLV integration\n",
        "        self.temporal_attention = MultiBandTemporalAttention(\n",
        "            seq_len, hidden_dim, num_heads, len(self.freq_bands)\n",
        "        )\n",
        "\n",
        "        # Separate GAT layers for each frequency band\n",
        "        self.band_gat_layers = nn.ModuleDict()\n",
        "        for band in self.freq_bands:\n",
        "            self.band_gat_layers[band] = nn.ModuleList([\n",
        "                GATConv(hidden_dim, hidden_dim, heads=num_heads, dropout=0.1, concat=False)\n",
        "                for _ in range(num_gat_layers)\n",
        "            ])\n",
        "\n",
        "        # Cross-band fusion\n",
        "        self.band_fusion = nn.Linear(hidden_dim * len(self.freq_bands), hidden_dim)\n",
        "\n",
        "        # Global pooling and classification\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_indices, plv_features, batch=None):\n",
        "        # x shape: (batch_size, n_channels, seq_len)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Apply multi-band temporal attention with PLV features\n",
        "        x_temporal = self.temporal_attention(x, plv_features)  # (batch_size, n_channels, hidden_dim)\n",
        "\n",
        "        # Process each frequency band separately\n",
        "        band_outputs = []\n",
        "        for band in self.freq_bands:\n",
        "            if band in edge_indices and edge_indices[band].size(1) > 0:\n",
        "                # Reshape for GAT processing\n",
        "                x_band = x_temporal.view(batch_size * self.n_channels, self.hidden_dim)\n",
        "\n",
        "                # Get edge index for this band\n",
        "                edge_index = edge_indices[band]\n",
        "\n",
        "                # Expand edge_index for batch processing\n",
        "                if batch is None:\n",
        "                    edge_index_batch = []\n",
        "                    for i in range(batch_size):\n",
        "                        offset = i * self.n_channels\n",
        "                        edge_index_batch.append(edge_index + offset)\n",
        "                    edge_index = torch.cat(edge_index_batch, dim=1)\n",
        "\n",
        "                # Apply GAT layers for this band\n",
        "                x_band_out = x_band\n",
        "                for gat_layer in self.band_gat_layers[band]:\n",
        "                    x_band_out = gat_layer(x_band_out, edge_index)\n",
        "                    x_band_out = F.relu(x_band_out)\n",
        "                    x_band_out = F.dropout(x_band_out, training=self.training)\n",
        "\n",
        "                # Reshape back to batch format\n",
        "                x_band_out = x_band_out.view(batch_size, self.n_channels, self.hidden_dim)\n",
        "                band_outputs.append(x_band_out)\n",
        "            else:\n",
        "                # If no edges for this band, use temporal features directly\n",
        "                band_outputs.append(x_temporal)\n",
        "\n",
        "        # Fuse information from all frequency bands\n",
        "        if len(band_outputs) > 1:\n",
        "            x_fused = torch.cat(band_outputs, dim=-1)  # (batch_size, n_channels, hidden_dim * num_bands)\n",
        "            x_fused = self.band_fusion(x_fused)  # (batch_size, n_channels, hidden_dim)\n",
        "        else:\n",
        "            x_fused = band_outputs[0]\n",
        "\n",
        "        # Global pooling across channels\n",
        "        x_fused = x_fused.transpose(1, 2)  # (batch_size, hidden_dim, n_channels)\n",
        "        x_pooled = self.global_pool(x_fused).squeeze(-1)  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(x_pooled)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class TMSEEGClassifier:\n",
        "    \"\"\"\n",
        "    Main classifier class that combines PLV-based data loading and model training\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.root_dir = root_dir\n",
        "        self.device = device\n",
        "        self.data_loader = EEGDataLoader(root_dir)\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load all EEG data\"\"\"\n",
        "        print(\"Loading EEG data...\")\n",
        "        self.data_loader.load_all_subjects()\n",
        "\n",
        "    def prepare_dataset(self, window_size=1000, overlap=0.5):\n",
        "        \"\"\"Prepare dataset for training with PLV-based connectivity\"\"\"\n",
        "        print(\"Preparing dataset with PLV-based connectivity...\")\n",
        "\n",
        "        # Combine TMS and resting data\n",
        "        combined_eeg = {}\n",
        "        combined_events = {}\n",
        "\n",
        "        # Add TMS data with events\n",
        "        for subject_id in self.data_loader.tms_data.keys():\n",
        "            combined_eeg[f\"{subject_id}_tms\"] = self.data_loader.tms_data[subject_id]\n",
        "            combined_events[f\"{subject_id}_tms\"] = self.data_loader.events_data.get(subject_id, None)\n",
        "\n",
        "        # Add resting data without events (background class)\n",
        "        for subject_id in self.data_loader.resting_data.keys():\n",
        "            combined_eeg[f\"{subject_id}_rest\"] = self.data_loader.resting_data[subject_id]\n",
        "            combined_events[f\"{subject_id}_rest\"] = None\n",
        "\n",
        "        # Create dataset with PLV-based connectivity\n",
        "        self.dataset = TMSEEGDataset(combined_eeg, combined_events, window_size, overlap)\n",
        "\n",
        "        print(f\"Created dataset with {len(self.dataset)} samples\")\n",
        "\n",
        "        # Get data dimensions\n",
        "        sample = self.dataset[0]\n",
        "        self.n_channels = sample['x'].shape[0]\n",
        "        self.seq_len = sample['x'].shape[1]\n",
        "\n",
        "        print(f\"Data dimensions: {self.n_channels} channels, {self.seq_len} time points\")\n",
        "\n",
        "    def create_model(self, hidden_dim=64, num_heads=8, num_gat_layers=2):\n",
        "        \"\"\"Create the PLV-based GAT model\"\"\"\n",
        "        self.model = PLVSpatioTemporalGAT(\n",
        "            n_channels=self.n_channels,\n",
        "            seq_len=self.seq_len,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_gat_layers=num_gat_layers\n",
        "        ).to(self.device)\n",
        "\n",
        "        print(f\"Created PLV-based model with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
        "\n",
        "    def train(self, epochs=100, batch_size=32, learning_rate=0.001):\n",
        "        \"\"\"Train the PLV-based model\"\"\"\n",
        "        if self.model is None:\n",
        "            self.create_model()\n",
        "\n",
        "        # Split dataset\n",
        "        train_size = int(0.8 * len(self.dataset))\n",
        "        val_size = len(self.dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            self.dataset, [train_size, val_size]\n",
        "        )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Optimizer and loss\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        print(\"Starting PLV-based training...\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch in train_loader:\n",
        "                x = batch['x'].to(self.device)\n",
        "                edge_indices = {k: v.to(self.device) for k, v in batch['edge_indices'][0].items()}\n",
        "                plv_features = batch['plv_features'][0]\n",
        "                y = torch.tensor(batch['y']).to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.model(x, edge_indices, plv_features)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(logits.data, 1)\n",
        "                train_total += y.size(0)\n",
        "                train_correct += (predicted == y).sum().item()\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    x = batch['x'].to(self.device)\n",
        "                    edge_indices = {k: v.to(self.device) for k, v in batch['edge_indices'][0].items()}\n",
        "                    plv_features = batch['plv_features'][0]\n",
        "                    y = torch.tensor(batch['y']).to(self.device)\n",
        "\n",
        "                    logits = self.model(x, edge_indices, plv_features)\n",
        "                    loss = criterion(logits, y)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(logits.data, 1)\n",
        "                    val_total += y.size(0)\n",
        "                    val_correct += (predicted == y).sum().item()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs}\")\n",
        "                print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {100*train_correct/train_total:.2f}%\")\n",
        "                print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {100*val_correct/val_total:.2f}%\")\n",
        "                print(\"-\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saJWcjPSXh7H"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize classifier\n",
        "    root_directory = \"EEGTMS\"  # Update this path\n",
        "    classifier = TMSEEGClassifier(root_directory)\n",
        "\n",
        "    # Load and prepare data\n",
        "    classifier.load_data()\n",
        "    classifier.prepare_dataset(window_size=1000, overlap=0.5)\n",
        "\n",
        "    # Train model\n",
        "    classifier.train(epochs=100, batch_size=16, learning_rate=0.001)\n",
        "    print(\"PLV-based training completed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}